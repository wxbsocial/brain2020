{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn related imports\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# pytorch relates imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3741dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a191d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./lookupcsv/ADNI.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2a30f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demors len: 35\n"
     ]
    }
   ],
   "source": [
    "names,y,X = utils.read_csv_copd(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b041c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb6132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8d3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['alff1',\"alff2\", \"alff3\",\"alff4\",\"alff5\",\"alff6\",\"alff7\",\"alff8\",\"alff9\",\"alff10\",\n",
    "    \"reho1\",\"reho2\",\"reho3\",\"reho4\",\"reho5\"\t,\"reho6\",\"reho7\",\"reho8\",\"reho9\",\"reho10\",\"reho11\",\"reho12\",\"reho13\",\"reho14\",\"reho15\",\t\n",
    "    \"vmhc1\",\"vmhc2\",\"vmhc3\",\"vmhc4\",\"vmhc5\",\"vmhc6\",\"vmhc7\",\"vmhc8\",\"vmhc9\",\"vmhc10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"alff4\",\"alff5\",\"alff7\",\"alff10\",\n",
    "    \"reho6\",\"reho14\",\"reho15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ebb55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 13, ncols=3, figsize=(30, 80))\n",
    "for i, (ax, col) in enumerate(zip(axs.flat, feature_names)):    \n",
    "    x = X[:,i]\n",
    "    pf = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(pf)\n",
    "\n",
    "    ax.plot(x, y, 'o')\n",
    "    ax.plot(x, p(x),\"r--\")\n",
    "\n",
    "    #ax.set_title(col + ' vs AE Value')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('AE Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e9a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_load_save_model(model_obj, model_path):\n",
    "    if path.isfile(model_path):\n",
    "        # load model\n",
    "        print('Loading pre-trained model from: {}'.format(model_path))\n",
    "        model_obj.load_state_dict(torch.load(model_path))\n",
    "    else:    \n",
    "        # train model\n",
    "        train(model_obj)\n",
    "        print('Finished training the model. Saving the model to the path: {}'.format(model_path))\n",
    "        torch.save(model_obj.state_dict(), model_path)\n",
    "\n",
    "def print_model(model_obj):\n",
    "    for parameters in model_obj.parameters():\n",
    "        print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoint_dir/mlp_COPD_exp0/mlp_COPD_14.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe349a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import model\n",
    "config= utils.read_json('./config.json')[\"mlp_COPD\"]\n",
    "print(config['drop_rate'])\n",
    "model = model._MLP_COPD(\n",
    "   7,\n",
    "   config['drop_rate'],\n",
    "   config['fil_num'],\n",
    "   )\n",
    "#print_model(model)\n",
    "train_load_save_model(model, model_path)\n",
    "#print_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77846ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = \"./lookupcsv/exp0/train.csv\"\n",
    "_, y_train,X_train = utils.read_csv_copd(train_path)\n",
    "test_path = \"./lookupcsv/exp0/test.csv\"\n",
    "_, y_test,X_test = utils.read_csv_copd(test_path)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "#err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "\n",
    "#print('model err: ', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f983529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)\n",
    "\n",
    "target_class_index = 1\n",
    "data = X_train\n",
    "baselines = torch.zeros(data.shape)\n",
    "ig_attr_test = ig.attribute(data, target=target_class_index,n_steps=8)\n",
    "ig_nt_attr_test = ig_nt.attribute(data, target=target_class_index)\n",
    "dl_attr_test = dl.attribute(data, target=target_class_index)\n",
    "gs_attr_test = gs.attribute(data, baselines, target=target_class_index)\n",
    "fa_attr_test = fa.attribute(data, target=target_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d0dd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare attributions for visualization\n",
    "\n",
    "x_axis_data = np.arange(X_test.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n",
    "dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n",
    "\n",
    "gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)\n",
    "gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)\n",
    "\n",
    "fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "lin_weight = model.net[1].weight[0].detach().numpy()\n",
    "y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\n",
    "width = 0.14\n",
    "legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'GradientSHAP', 'Feature Ablation', 'Weights']\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "ax.set_ylabel('Attributions')\n",
    "\n",
    "FONT_SIZE = 16\n",
    "plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n",
    "ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n",
    "ax.bar(x_axis_data + 3 * width, gs_attr_test_norm_sum, width, align='center',  alpha=0.8, color='#4260f5')\n",
    "ax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "ax.bar(x_axis_data + 5 * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\n",
    "ax.autoscale_view()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_xticks(x_axis_data + 0.5)\n",
    "ax.set_xticklabels(x_axis_data_labels)\n",
    "\n",
    "plt.legend(legends, loc=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
